{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpUMJnzgHlQft+KBVbbZXc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **STROKE AND HEALTH DEMOGRAPHICS**:"],"metadata":{"id":"pi8pqsqcYog_"}},{"cell_type":"markdown","source":["# Part 1: Subset K-Means Clustering for Health Profile Segmentation"],"metadata":{"id":"GXtfssylYqlL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nU7q4CCvyZk7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715872471997,"user_tz":-120,"elapsed":18249,"user":{"displayName":"Sara M","userId":"02845766730570862326"}},"outputId":"2a3a2e4e-31a7-4d22-b247-a4d0c2fa1fc9"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-3dea7c4d56f8>:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n","  from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import glob\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import os\n","import os,glob\n","import tensorflow as tf\n","from zipfile import ZipFile\n","import os,glob\n","import cv2\n","from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n","import numpy as np\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Convolution2D, Dropout, Dense,MaxPooling2D\n","from keras.layers import BatchNormalization\n","from keras.layers import MaxPooling2D\n","from keras.layers import Flatten\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.datasets import make_blobs\n","import seaborn as sns\n","import sys"]},{"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"HbI4UnUDzjlz","executionInfo":{"status":"ok","timestamp":1715873073797,"user_tz":-120,"elapsed":13221,"user":{"displayName":"Sara M","userId":"02845766730570862326"}},"outputId":"459d21c7-b550-4fb9-f9ab-0c6c5a5275a4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c12e5b24-d5c7-4ea6-a000-c7ebf42f30fe\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c12e5b24-d5c7-4ea6-a000-c7ebf42f30fe\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Stroke Subset.csv to Stroke Subset (1).csv\n"]}]},{"cell_type":"code","source":["dataset = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')"],"metadata":{"id":"JF99OKx20SEd","executionInfo":{"status":"error","timestamp":1715873099794,"user_tz":-120,"elapsed":709,"user":{"displayName":"Sara M","userId":"02845766730570862326"}},"colab":{"base_uri":"https://localhost:8080/","height":200},"outputId":"35ca255d-7f5a-4bec-b117-4a3a2db6019b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-de4eb7b8d032>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Stroke Subset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n"]}]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"Fd413d9O05GA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.describe()"],"metadata":{"id":"tXRMlph31oAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","import numpy as np\n","\n","# Assuming 'dataset' is your DataFrame and 'age' is the column of interest\n","age_data = dataset['age']\n","mean_age = age_data.mean()\n","\n","# Create the histogram\n","n, bins, patches = plt.hist(age_data, bins=20, edgecolor='black')\n","\n","# Color the bins based on their distance from the mean\n","# Calculate the color intensity based on the closeness to the mean\n","for i, patch in enumerate(patches):\n","    bin_center = (bins[i] + bins[i+1]) / 2\n","    color_intensity = 1 - abs(bin_center - mean_age) / max(abs(bins - mean_age))\n","    patch.set_facecolor(plt.cm.viridis(color_intensity))\n","\n","# Add titles and labels\n","plt.title('Age Distribution with Gradient Color')\n","plt.xlabel('Age')\n","plt.ylabel('Frequency')\n","\n","# Hide top and right spines\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"L9cyKCeT2Qwk"},"execution_count":null,"outputs":[]},{"source":["# @title gender\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","dataset.groupby('gender').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"ZP4bhM771LhF"}},{"source":["# @title work_type\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","dataset.groupby('work_type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"8OrOETZ91O8p"}},{"source":["# @title Residence_type vs smoking_status\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","plt.subplots(figsize=(8, 8))\n","df_2dhist = pd.DataFrame({\n","    x_label: grp['smoking_status'].value_counts()\n","    for x_label, grp in dataset.groupby('Residence_type')\n","})\n","sns.heatmap(df_2dhist, cmap='viridis')\n","plt.xlabel('Residence_type')\n","_ = plt.ylabel('smoking_status')"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"mbJbZRUC1WkD"}},{"cell_type":"markdown","source":["# Experimental random subset"],"metadata":{"id":"tyGGbyEg7c8-"}},{"cell_type":"code","source":["# Set the random seed for reproducibility\n","np.random.seed(42)\n","\n","# Randomly choose 500 indices without replacement\n","subset_indices = np.random.choice(dataset.index, 500, replace=False)\n","\n","# Create the subset for clustering\n","subset = dataset.loc[subset_indices]"],"metadata":{"id":"dvsLyu_Zyqjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove rows with missing data from the subset\n","subset_cleaned = subset.dropna()\n","print(subset_cleaned.head())"],"metadata":{"id":"sEpRCdJ5B_Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset = subset_cleaned"],"metadata":{"id":"mRx8PdDR-mXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset"],"metadata":{"id":"PA2sU0qa7Tch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset.describe()"],"metadata":{"id":"TqKheLZ08TW2"},"execution_count":null,"outputs":[]},{"source":["age_data = subset['age']\n","mean_age = age_data.mean()\n","\n","# Create the histogram\n","n, bins, patches = plt.hist(age_data, bins=20, edgecolor='black')\n","\n","# Color the bins based on their distance from the mean\n","# Calculate the color intensity based on the closeness to the mean\n","for i, patch in enumerate(patches):\n","    bin_center = (bins[i] + bins[i+1]) / 2\n","    color_intensity = 1 - abs(bin_center - mean_age) / max(abs(bins - mean_age))\n","    patch.set_facecolor(plt.cm.viridis(color_intensity))\n","\n","# Add titles and labels\n","plt.title('Age Distribution with Gradient Color')\n","plt.xlabel('Age')\n","plt.ylabel('Frequency')\n","\n","# Hide top and right spines\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","\n","# Show the plot\n","plt.show()"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"G1K_iofD8Nm7"}},{"source":["# @title gender\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","subset.groupby('gender').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"VrwqOQko8QtU"}},{"cell_type":"markdown","source":["Subset of Data for Clustering"],"metadata":{"id":"Xu4osGuBpubL"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.decomposition import PCA"],"metadata":{"id":"PJYNZR-B7W8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_for_clustering = subset[['avg_glucose_level', 'bmi']]"],"metadata":{"id":"_FlgR2BH-IlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the transformations for the numerical and categorical data\n","numerical_features = ['age', 'avg_glucose_level', 'bmi']\n","categorical_features = ['gender', 'hypertension', 'heart_disease', 'ever_married',\n","                        'work_type', 'Residence_type', 'smoking_status']\n","\n","# Create transformers\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_features),\n","        ('cat', OneHotEncoder(), categorical_features)\n","    ])"],"metadata":{"id":"xqncVsgv_5Sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transformers for preprocessing\n","numerical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n","    ('scaler', StandardScaler())  # Scale numerical features\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing categorical data\n","    ('encoder', OneHotEncoder(handle_unknown='ignore')),  # Encode categorical features\n","\n","])"],"metadata":{"id":"ULm3T1N7DMUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["   # One-hot encoding for categorical variables\n","subset_encoded = pd.get_dummies(subset, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'])"],"metadata":{"id":"W73XKHz7IH6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(subset.columns) #check columns"],"metadata":{"id":"_MESGgnyqM_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_encoded.head() #check transform"],"metadata":{"id":"61D4U8fPITWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","# Initialize and fit the KMeans model\n","kmeans = KMeans(n_clusters=4, random_state=0)  # Adjust 'n_clusters' as necessary\n","subset['Cluster Membership'] = kmeans.fit_predict(subset[['avg_glucose_level', 'age']])\n"],"metadata":{"id":"KvthWREN99SW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Scatter plot for visualizing clusters based on two features\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(subset, x='avg_glucose_level', y='age', hue='Cluster Membership', style='Cluster Membership', palette='viridis')\n","plt.title('K-Means Clustering Results')\n","plt.xlabel('avg_glucose_level')\n","plt.ylabel('age')\n","plt.legend(title='Cluster')\n","plt.show()\n","\n","# Display basic statistics per cluster\n","print(subset.groupby('Cluster Membership').mean())"],"metadata":{"id":"geBTCCX-66az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine preprocessing steps into a single transformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Create a pipeline that includes preprocessing and K-Means clustering\n","pipeline = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('cluster', KMeans(n_clusters=4))\n","])\n","\n","# Fit the pipeline to the data\n","pipeline.fit(subset)\n","\n","# Retrieve the cluster labels\n","kmeans_labels = pipeline.named_steps['cluster'].labels_\n","\n","# Add the cluster labels to your DataFrame\n","subset['Cluster Membership'] = kmeans_labels\n","\n","# Display the first few entries of your DataFrame to see the clusters\n","print(\"First few rows of the subset with cluster memberships:\")\n","print(subset.head())"],"metadata":{"id":"ahSzy74DBLrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset['Cluster Membership'] = kmeans_labels"],"metadata":{"id":"-QoD_TOZ_h_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 'pipeline_features' is already defined as the preprocessed data\n","pipeline_features = pipeline.named_steps['preprocessor'].transform(subset)\n","\n","# Elbow Method: Compute the sum of squared distances for different values of k\n","sse = []\n","silhouette_scores = []\n","k_list = range(2, 10)  # Testing from 2 to 10 clusters\n","for k in k_list:\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(pipeline_features)\n","    sse.append(kmeans.inertia_)  # Sum of squared distances to closest cluster center\n","    labels = kmeans.labels_\n","    silhouette_scores.append(silhouette_score(pipeline_features, labels))\n","\n","# Plotting the Elbow Curve\n","plt.figure(figsize=(10, 5))\n","plt.plot(k_list, sse, marker='o')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Sum of squared distances')\n","plt.title('Elbow Method For Optimal k')\n","plt.show()\n","\n","# Plotting Silhouette Scores\n","plt.figure(figsize=(10, 5))\n","plt.plot(k_list, silhouette_scores, marker='o', color='red')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Scores For Different k')\n","plt.show()\n","\n","# Choose a new number of clusters based on analysis\n","new_k = 5  # Example: let's assume 5 is chosen\n","\n","# Re-run KMeans with the new number of clusters\n","kmeans = KMeans(n_clusters=new_k, random_state=42)\n","kmeans.fit(pipeline_features)\n","labels = kmeans.labels_\n","\n","# PCA for visualization\n","pca = PCA(n_components=2)\n","reduced_data = pca.fit_transform(pipeline_features)\n","\n","# Plot\n","plt.figure(figsize=(10, 8))\n","plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis', marker='o', alpha=0.5)\n","plt.colorbar(ticks=range(new_k))\n","plt.xlabel('PCA 1')\n","plt.ylabel('PCA 2')\n","plt.title(f'Cluster Visualization with {new_k} Clusters')\n","plt.show()"],"metadata":{"id":"w_lAHeEpELRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#appending to df and validating\n","subset['subset_cluster'] = labels\n","subset.head()"],"metadata":{"id":"cTp0imkK-4iQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare to a Biplot for Loadings"],"metadata":{"id":"8KLEsYUA6c8h"}},{"cell_type":"code","source":["# Extract the PCA instance if you already have it; otherwise, initialize and fit it here\n","pca = PCA(n_components=2)\n","reduced_data = pca.fit_transform(pipeline_features)  # This line can be omitted if PCA is already done\n","\n","# Extract loadings\n","loadings = pca.components_.T\n","\n","# Plotting the PCA results\n","fig, ax = plt.subplots(figsize=(10, 8))\n","\n","# Plotting the scores (data points)\n","scatter = ax.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis', alpha=0.5)\n","\n","# Plotting the loadings (variable vectors)\n","for i, loading in enumerate(loadings):\n","    ax.arrow(0, 0, loading[0] * max(reduced_data[:,0]), loading[1] * max(reduced_data[:,1]), color='red', alpha=0.5)\n","    ax.text(loading[0] * max(reduced_data[:,0]) * 1.1, loading[1] * max(reduced_data[:,1]) * 1.1, f'Variable {i+1}', color='red')\n","\n","# Adding labels and title\n","ax.set_xlabel('Principal Component 1')\n","ax.set_ylabel('Principal Component 2')\n","ax.set_title('PCA Biplot with Loadings and Data Points')\n","plt.colorbar(scatter, ticks=range(len(np.unique(labels))), label='Cluster Label')\n","\n","# Show plot\n","plt.show()"],"metadata":{"id":"qTOOxhRp6Ry6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Heatmap for PCA Loadings"],"metadata":{"id":"HSYooJsC7zmo"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# Assuming 'pipeline_features' is your data\n","pca = PCA(n_components=5)  # Adjust based on how many components you want to analyze\n","reduced_data = pca.fit_transform(pipeline_features)\n","loadings = pca.components_.T  # Transpose so rows are variables and columns are components\n","\n","# If you know the feature names\n","feature_names = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'etc']  # Replace with actual names\n","\n","# Create a heatmap of the loadings\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(loadings, cmap=\"viridis\", annot=True, fmt=\".2f\", xticklabels=[f'PC{i+1}' for i in range(loadings.shape[1])], yticklabels=feature_names)\n","plt.title('Heatmap of PCA Loadings')\n","plt.xlabel('Principal Components')\n","plt.ylabel('Features')\n","plt.show()\n"],"metadata":{"id":"bZe5Yz9P7v6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#appending to df and validating\n","subset['subset_cluster'] = labels\n","subset.head()"],"metadata":{"id":"4Pjt-nLdARK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","\n","# Assuming 'df' is your original DataFrame and 'pipeline_features' is the processed data used for clustering\n","pca = PCA(n_components=5)\n","pipeline_features = pca.fit_transform(subset)\n","\n","# Fit KMeans (or your clustering model)\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","cluster_labels = kmeans.fit_predict(pipeline_features)\n","\n","# Add cluster labels to your DataFrame\n","df['Cluster'] = cluster_labels\n","\n","# Group data by cluster and calculate descriptive statistics\n","cluster_description = df.groupby('Cluster').describe()\n","\n","# To print or analyze descriptives for each cluster and each variable\n","for cluster_num in range(kmeans.n_clusters):\n","    print(f\"Descriptive Statistics for Cluster {cluster_num}:\")\n","    print(cluster_description.xs(cluster_num, level='Cluster'))  # Use .xs to select a particular cluster\n","    print(\"\\n\")"],"metadata":{"id":"aSQuhoFR9Fau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset['Cluster Membership'] = labels"],"metadata":{"id":"013lptEwE9Ek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary to hold your segments\n","clusters = {}\n","num_clusters = subset['Cluster Membership'].nunique()  # or set manually the number of clusters used\n","\n","for i in range(num_clusters):\n","    clusters[f\"cluster_{i}\"] = subset[subset['Cluster Membership'] == i]"],"metadata":{"id":"Tt2lrJqME9B5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print basic statistics for each cluster\n","for i in range(num_clusters):\n","    print(f\"Statistics for cluster {i}:\")\n","    print(clusters[f\"cluster_{i}\"].describe(), \"\\n\")"],"metadata":{"id":"pvOYduT0E9AO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# For example, let's visualize the distribution of 'age' across clusters\n","plt.figure(figsize=(10, 6))\n","for i in range(num_clusters):\n","    sns.distplot(clusters[f\"cluster_{i}\"]['age'], label=f'Cluster {i}')\n","plt.title('Age Distribution by Cluster')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"pGUc2yTlE89e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 'subset' already has 'Cluster Membership' assigned\n","num_clusters = subset['Cluster Membership'].nunique()  # get the number of unique clusters\n","\n","clusters = {i: subset[subset['Cluster Membership'] == i] for i in range(num_clusters)}"],"metadata":{"id":"hqxIDVmdE84p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# List of variables you are interested in visualizing\n","variables_to_plot = ['age', 'avg_glucose_level', 'bmi']\n","\n","# Plotting distributions or boxplots for each variable\n","for variable in variables_to_plot:\n","    plt.figure(figsize=(10, 6))\n","    for i in range(num_clusters):\n","        sns.distplot(clusters[i][variable], hist=False, kde=True,\n","                     kde_kws={'shade': True},\n","                     label=f'Cluster {i}')\n","    plt.title(f'Distribution of {variable} by Cluster')\n","    plt.legend()\n","    plt.show()\n","\n","    # Boxplot for the same variable\n","    plt.figure(figsize=(10, 6))\n","    sns.boxplot(x='Cluster Membership', y=variable, data=subset)\n","    plt.title(f'Boxplot of {variable} by Cluster')\n","    plt.show()"],"metadata":{"id":"FDPcrtWvF_dG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(kmeans_labels))  # Number of cluster labels\n","print(len(dataset))        # Number of rows in the dataset\n","\n","# 477=subset n, 5110=dataset (df) n\n","# alignment needed if cluster assignment to dataset"],"metadata":{"id":"DhJlpDJ75dyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Saving subset df as an excel sheet for further analyses and visualizations of the new cluster memberships"],"metadata":{"id":"DyVBN-CaM5yc"}},{"cell_type":"code","source":["pip install xlsxwriter\n"],"metadata":{"id":"HrM9vEeq3vms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the file path and name for the Excel file\n","file_path = 'subset_with_clusters_stroke_df.xlsx'\n","\n","# Save the DataFrame to an Excel file\n","subset.to_excel(file_path, index=False)  # 'index=False' to avoid saving the index as a column in the Excel file\n","\n","print(f\"The subset DataFrame has been successfully saved to {file_path}\")"],"metadata":{"id":"tim4kBtE3vkc"},"execution_count":null,"outputs":[]}]}